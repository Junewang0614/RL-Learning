{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1282ee9f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 马尔可夫决策过程\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f7a43f3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 马尔可夫奖励过程\n",
    "# 需要的变量\n",
    "# 状态集合\n",
    "# 状态转移概率（矩阵）\n",
    "# 奖励函数\n",
    "# 折扣因子\n",
    "\n",
    "# 状态有6个\n",
    "# 状态转移概率矩阵\n",
    "P = [\n",
    "    [0.9, 0.1, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.6, 0.0, 0.4],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.3, 0.7],\n",
    "    [0.0, 0.2, 0.3, 0.5, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "] \n",
    "P = np.array(P)\n",
    "\n",
    "# 对应的奖励函数\n",
    "rewards = [-1, -2, -2, 10, 1, 0]\n",
    "# 折扣因子\n",
    "gamma = 0.5  # 定义折扣因子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24295ec3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 回报G的计算，给定一条序列,计算从某个索引（起始状态）开始到序列最后（终止状态）得到的回报\n",
    "# G之间的递推关系\n",
    "# Gt = Rt + gamma * G(t+1)\n",
    "def compute_return(start_index,chain,gamma,rewards):\n",
    "    G = 0\n",
    "    # 所以这里是倒叙\n",
    "    for i in reversed(range(start_index,len(chain))):\n",
    "        G = gamma * G + rewards[chain[i] - 1]\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d141597",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据本序列计算得到回报为：3.0。\n"
     ]
    }
   ],
   "source": [
    "# 计算回报G\n",
    "\n",
    "# 序列\n",
    "chain = [1,2,3,4,5,3,6]\n",
    "start_index = 2\n",
    "G = compute_return(start_index,chain,gamma,rewards)\n",
    "print(\"根据本序列计算得到回报为：%s。\" % G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e5a3b53",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 贝尔曼方程求解马尔可夫奖励的价值函数\n",
    "def compute(P,rewards,gammma):\n",
    "    ''' 利用贝尔曼方程的矩阵形式计算解析解,states_num是MRP的状态数 '''\n",
    "    # V = R + gamma * PV\n",
    "    # V = （I-gammaP）^-1R\n",
    "    states_num = len(rewards)\n",
    "    rewards = np.array(rewards).reshape((-1,1))\n",
    "    temp = np.eye(states_num,states_num) # 生成num*num的对角阵\n",
    "    temp = temp - gamma * P # [num,num]\n",
    "    # 求矩阵的逆\n",
    "    temp = np.linalg.inv(temp) # [num,num]\n",
    "    # 矩阵乘法\n",
    "    value = np.dot(temp,rewards)\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "456857b6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRP中每个状态价值分别为\n",
      " [[-2.01950168]\n",
      " [-2.21451846]\n",
      " [ 1.16142785]\n",
      " [10.53809283]\n",
      " [ 3.58728554]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# 计算价值函数\n",
    "# 这个在确定的P,REWARDS,GAMMA下就是确定的，和具体的序列没有关系\n",
    "V = compute(P, rewards, gamma)\n",
    "print(\"MRP中每个状态价值分别为\\n\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60492969",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 马尔可夫决策过程的表示\n",
    "# S,A,P,gamma,rewards\n",
    "S = [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"]  # 状态集合\n",
    "A = [\"保持s1\", \"前往s1\", \"前往s2\", \"前往s3\", \"前往s4\", \"前往s5\", \"概率前往\"]  # 动作集合\n",
    "\n",
    "# 状态转移函数\n",
    "# 格式：当前状态-采取的行动-转移的状态\n",
    "P = {\n",
    "    \"s1-保持s1-s1\": 1.0,\n",
    "    \"s1-前往s2-s2\": 1.0,\n",
    "    \"s2-前往s1-s1\": 1.0,\n",
    "    \"s2-前往s3-s3\": 1.0,\n",
    "    \"s3-前往s4-s4\": 1.0,\n",
    "    \"s3-前往s5-s5\": 1.0,\n",
    "    \"s4-前往s5-s5\": 1.0,\n",
    "    \"s4-概率前往-s2\": 0.2,\n",
    "    \"s4-概率前往-s3\": 0.4,\n",
    "    \"s4-概率前往-s4\": 0.4,\n",
    "}\n",
    "\n",
    "# 奖励函数\n",
    "# 格式：当前状态-采取行动\n",
    "R = {\n",
    "    \"s1-保持s1\": -1,\n",
    "    \"s1-前往s2\": 0,\n",
    "    \"s2-前往s1\": -1,\n",
    "    \"s2-前往s3\": -2,\n",
    "    \"s3-前往s4\": -2,\n",
    "    \"s3-前往s5\": 0,\n",
    "    \"s4-前往s5\": 10,\n",
    "    \"s4-概率前往\": 1,\n",
    "}\n",
    "gamma = 0.5\n",
    "\n",
    "# 一个马尔科夫决策过程\n",
    "MDP = (S,A,P,R,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b264c9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 策略定义\n",
    "\n",
    "# 1. 完全随机策略\n",
    "# 即在每个状态下，智能体会以同样的概率选取它可能采取的动作。\n",
    "Pi_1 = {\n",
    "    \"s1-保持s1\": 0.5,\n",
    "    \"s1-前往s2\": 0.5,\n",
    "    \"s2-前往s1\": 0.5,\n",
    "    \"s2-前往s3\": 0.5,\n",
    "    \"s3-前往s4\": 0.5,\n",
    "    \"s3-前往s5\": 0.5,\n",
    "    \"s4-前往s5\": 0.5,\n",
    "    \"s4-概率前往\": 0.5,\n",
    "}\n",
    "\n",
    "# 2.有一定设置的策略\n",
    "Pi_2 = {\n",
    "    \"s1-保持s1\": 0.6,\n",
    "    \"s1-前往s2\": 0.4,\n",
    "    \"s2-前往s1\": 0.3,\n",
    "    \"s2-前往s3\": 0.7,\n",
    "    \"s3-前往s4\": 0.5,\n",
    "    \"s3-前往s5\": 0.5,\n",
    "    \"s4-前往s5\": 0.1,\n",
    "    \"s4-概率前往\": 0.9,\n",
    "}\n",
    "\n",
    "# util\n",
    "def join(str1, str2):\n",
    "    return str1 + '-' + str2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ead45894",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 计算该 MDP 下，一个策略的状态价值函数。\n",
    "# !!!转化前的 MDP 的状态价值函数和转化后的 MRP 的价值函数是一样的。!!!  ？？？\n",
    "# 1. 将MDP中的action边缘化，转换为MRP\n",
    "R_from_mdp_to_mrp = [-0.5, -1.5, -1.0, 5.5, 0] # 边缘化后的奖励函数\n",
    "# 转化后的MRP的状态转移矩阵\n",
    "P_from_mdp_to_mrp = [\n",
    "    [0.5, 0.5, 0.0, 0.0, 0.0],\n",
    "    [0.5, 0.0, 0.5, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.5, 0.5],\n",
    "    [0.0, 0.1, 0.2, 0.2, 0.5],\n",
    "    [0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "]\n",
    "P_from_mdp_to_mrp = np.array(P_from_mdp_to_mrp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "410d8205",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP中每个状态价值分别为\n",
      " [[-1.22555411]\n",
      " [-1.67666232]\n",
      " [ 0.51890482]\n",
      " [ 6.0756193 ]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# 2. 根据MRP计算value-state 的方法，计算,这个可以当中标准来对照其他方法\n",
    "# 知道了value-state函数，就能够计算action-value函数\n",
    "value = compute(P_from_mdp_to_mrp,R_from_mdp_to_mrp,gamma)\n",
    "print(\"MDP中每个状态价值分别为\\n\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0770b21d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 蒙特卡洛方法\n",
    "# 这是用来计算value-state函数\n",
    "# 1. 采样\n",
    "def sample(MDP,Pi,timestep_max,number):\n",
    "    '''\n",
    "    在策略pi的条件下，最长时间步为timestep_max,采样number组序列\n",
    "    '''\n",
    "    S, A, P, R, gamma = MDP\n",
    "    episodes = []\n",
    "    \n",
    "    for _ in range(number):\n",
    "        episode = []\n",
    "        timestep = 0\n",
    "        s = S[np.random.randint(4)] # 随机选择一个结束状态以外的状态作为起始状态\n",
    "        \n",
    "        # 沿着状态开始采样，当状态为终止状态或者时间步太长时,一次采样结束\n",
    "        while s != 's5' and timestep <= timestep_max:\n",
    "            timestep += 1\n",
    "            \n",
    "            # 根据策略选择状态s下的动作\n",
    "            rand,temp = np.random.rand(),0\n",
    "            # rand是随机一个概率\n",
    "            # 感觉有点问题，但也能够保证随机性。。\n",
    "            for a_opt in A:\n",
    "                temp += Pi.get(join(s,a_opt),0)\n",
    "                # 确定动作action\n",
    "                if temp > rand:\n",
    "                    a = a_opt\n",
    "                    r = R.get(join(s,a),0)\n",
    "                    break\n",
    "            \n",
    "            # 根据状态转移概率得到下一个状态\n",
    "            rand,temp = np.random.rand(),0\n",
    "            for s_opt in S:\n",
    "                temp += P.get(join(join(s,a),s_opt),0)\n",
    "                if temp > rand:\n",
    "                    # 确定了下一个状态\n",
    "                    s_next = s_opt\n",
    "                    break\n",
    "            \n",
    "            # 至此，由s和pai，获得了 a，r还有下一个状态s_next\n",
    "            episode.append((s,a,r,s_next)) \n",
    "            s = s_next\n",
    "        episodes.append(episode)\n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1c1c38d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 计算state-value\n",
    "def MC(episodes,V,N,gamma):\n",
    "    '''\n",
    "    episodes：采样的序列\n",
    "    V：用于维护状态的总回报\n",
    "    N：用于维护状态的计数器\n",
    "    '''\n",
    "    for episode in episodes:\n",
    "        G = 0 # 回报，累计奖励\n",
    "        # 为了增量更新，序列从后往前计算\n",
    "        for i in range(len(episode)-1,-1,-1):\n",
    "            (s,a,r,s_next) = episode[i]\n",
    "            # 该时间步内的回报\n",
    "            G = r + gamma*G\n",
    "            N[s] = N[s] + 1 # 更新状态的计数\n",
    "            V[s] = V[s] + (G - V[s])/N[s]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff9a98fc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 采样\n",
    "timestep_max = 20\n",
    "episodes = sample(MDP,Pi_1,timestep_max,500)\n",
    "gamma = 0.5\n",
    "V = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\n",
    "N = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\n",
    "MC(episodes,V,N,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16b9e568",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 占用度量的计算\n",
    "# 占用度量用来描述一个策略函数在MDP中的对不同的（s,a）访问概率\n",
    "def occupancy(episodes,s,a,timesstep_max,gamma):\n",
    "    ''' 计算状态动作对（s,a）出现的频率,以此来估算策略的占用度量 '''\n",
    "    rho = 0\n",
    "    total_times = np.zeros(timestep_max) # 记录每个时间步t各被经历过几次\n",
    "    occur_times = np.zeros(timestep_max) # 记录(s_t,a_t)=(s,a)的次数，访问特定状态动作对的次数\n",
    "    \n",
    "    # 扫描所有的采样，然后进行状态动作对和时间步的计数\n",
    "    for epi in episodes:\n",
    "        for i in range(len(epi)):\n",
    "            # 一个探索序列内\n",
    "            (s_opt,a_opt,r,s_next) = epi[i]\n",
    "            total_times[i] += 1   # 在\n",
    "            \n",
    "            if s == s_opt and a == a_opt:\n",
    "                occur_times[i] += 1\n",
    "    \n",
    "    # 累加求和，计算占用度量\n",
    "    for i in range(timestep_max):\n",
    "        if total_times[i]:\n",
    "            # 该时间步曾经出现过\n",
    "            # 后半部分的表示有些不明白？？？\n",
    "            rho += (gamma ** i) * (occur_times[i] / total_times[i])\n",
    "    \n",
    "    return (1 - gamma) * rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27b95717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10328751700335134 0.22613134269430213\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.5\n",
    "timestep_max = 1000\n",
    "episodes_1 = sample(MDP, Pi_1, timestep_max, 1000) # 策略1的采样\n",
    "episodes_2 = sample(MDP, Pi_2, timestep_max, 1000) # 策略2的采样\n",
    "\n",
    "# 这里查看（s4,概率前往）动作对的情况\n",
    "rho_1 = occupancy(episodes_1,\"s4\",\"概率前往\",timestep_max, gamma)\n",
    "rho_2 = occupancy(episodes_2,\"s4\",\"概率前往\",timestep_max, gamma)\n",
    "print(rho_1, rho_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40a5478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7993003a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
